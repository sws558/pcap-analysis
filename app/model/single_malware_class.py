from keras.models import Model  #16分类模型
from keras.layers import Dense, Input, Dropout, MaxPooling1D, Conv1D, GlobalMaxPool1D, Activation
from keras.layers import LSTM, Lambda, Bidirectional, concatenate, BatchNormalization
from keras.layers import TimeDistributed
from keras.optimizers import Adam
from keras.utils import np_utils
import keras.backend as K
import numpy as np
import tensorflow as tf
import keras.callbacks
import sys
import os
from timeit import default_timer as timer
import glob
import time
import sklearn.metrics as metrics
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import datetime
from AttentionNetwork import HierarchicalAttentionNetwork
# from LossHistry import LossHistory
# from createMatrix import plot_Matrix,useGpus

from keras import initializers
# from keras.engine.topology import Layer
from keras.layers import Layer
from keras.layers import Dense, Input
from keras.layers import Embedding, GRU, Bidirectional, TimeDistributed

now_time = datetime.datetime.now()
data_path = '/home/CIC2017/process_CIC2017_Data_single_malware_containedValid_part_10-23'
PACKET_NUM_PER_SESSION = 30  #readpatch的前n个数据包
PACKET_LEN = 300     #前n个数据点
BATCH_SIZE = 32     #可修改
LSTM_UNITS = 92     #没用到？？？
CHECKPOINTS_DIR = '/home/iscx2012_cnn_rnn_5class_new_checkpoints/'
TRAIN_STEPS_PER_EPOCH = 1956#2336#1168 #74793  每个epoch分成多少个batch，注意！！！！！！！！！！！！！！！！！！！！！！#没用到？？？
TEST_STEPS_PER_EPOCH = 232#292#146   #没用到？？？？
VALIDATION_STEPS_PER_EPOCH = 232#292#146 #28046
TRAIN_EPOCHS = 40  #没用？？？
true_labels = []
train_time = 0

#读取数据集,datatype是在数据集中key值
def readBatch(dataType):
    print("开始读取数据")
    print(dataType)
    if dataType == "trainData":
        path = data_path+"/train_parquet.npz"
    elif dataType == "validData":
        path = data_path+"/valid_parquet.npz"
#         dataType = "testData"
    else:
        path = data_path+"/test_parquet.npz"
    data = np.load(path,allow_pickle=True)
    malware_labels = data["malware_label"]    #多分类
    print(malware_labels.shape)
    pcapdata = data[dataType]
    pcapdata = pcapdata[:,:PACKET_NUM_PER_SESSION,:PACKET_LEN]*255
    pcapdata_reshape = pcapdata.reshape((-1,PACKET_NUM_PER_SESSION,PACKET_LEN))  #数据输入转换成hast输入格式，(BATCH_SIZE, PACKET_NUM_PER_SESSION, PACKET_LEN)
    print("训练集截取后的长度")
    #这里需要进行一层转换，因为存储的时候没有对类别进行reshape
    malware_labels=malware_labels.reshape((-1))
    if dataType == "testData":
        global true_labels
        true_labels = malware_labels
    malware_labels=np.eye(16)[malware_labels]
    return pcapdata_reshape,malware_labels

def mini_batch_generator(dataType):
    X,Y = readBatch(dataType)
    total_size = X.shape[0]
    batch_idx = 0
    while True:
        for i in range(total_size//BATCH_SIZE):
            yield X[i*BATCH_SIZE:(i+1)*BATCH_SIZE], Y[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
def binarize(x, sz=256):
    return tf.cast(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1),dtype=tf.float32)

def binarize_outshape(in_shape):
    return in_shape[0], in_shape[1], 256
def byte_block(in_layer, nb_filter=(64, 100), filter_length=(3, 3), subsample=(2, 1), pool_length=(2,2)):
    block = in_layer
    for i in range(len(nb_filter)):
        block = Conv1D(filters=nb_filter[i],
                       kernel_size=filter_length[i],
                       padding='valid',
                       activation='tanh',
                       strides=subsample[i])(block)
        if pool_length[i]:
            block = MaxPooling1D(pool_size=pool_length[i])(block)

    return block


train_data_generator = mini_batch_generator("trainData")
test_data_generator  = mini_batch_generator("testData")
valid_data_generator  = mini_batch_generator("validData")
print("数据读取完毕")


def makeModel():#第一个模型？？？
    session = Input(shape=(PACKET_NUM_PER_SESSION, PACKET_LEN), dtype='int64')
    input_packet = Input(shape=(PACKET_LEN,), dtype='int64')
    embedded = Lambda(binarize, output_shape=binarize_outshape)(input_packet)
    block = byte_block(embedded, (192, 320), filter_length=(6, 5), subsample=(1, 1), pool_length=(2,2))
    attn_package = HierarchicalAttentionNetwork(192)(block)
    encoder = Model(inputs=input_packet, outputs=attn_package)
    encoder.summary()

    review_encoder = TimeDistributed(encoder)(session)
    gru_flow = Bidirectional(GRU(512, return_sequences=True,dropout = 0.1,recurrent_dropout=0.1))(review_encoder)
    attn_flow = HierarchicalAttentionNetwork(512)(gru_flow)
    preds = Dense(16, activation='softmax')(attn_flow)#16类别的分类？
    model = Model(outputs=preds, inputs=session)
    model.summary()
    return model


def train(model):#加载预训练模型权重，进行模型编译和测试数据的预测，然后计算并输出模型的性能指标
    model.load_weights("./save_model/_300_30_single_malware.hdf5")
    weight_file = CHECKPOINTS_DIR  + '_' + str(PACKET_LEN) + '_' + str(PACKET_NUM_PER_SESSION)+str(now_time) + '_{epoch:02d}.hdf5'
    check_cb = keras.callbacks.ModelCheckpoint(weight_file, monitor='accuracy', verbose=0, save_best_only=True, save_weights_only=True, mode='max')
    earlystop_cb = keras.callbacks.EarlyStopping(monitor='accuracy', patience=20, verbose=0, mode='max')
    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    start = timer()
    #创建一个实例history，绘图需要
    # history = LossHistory()
    # model.fit_generator(
    #     generator=train_data_generator, 
    #     steps_per_epoch=TRAIN_STEPS_PER_EPOCH,
    #     epochs=TRAIN_EPOCHS,
    #     validation_data=valid_data_generator,
    #     validation_steps=VALIDATION_STEPS_PER_EPOCH,
    #     callbacks=[check_cb, earlystop_cb,history]
    # )
    end = timer()
    predictions = model.predict_generator(test_data_generator,verbose=1, steps=VALIDATION_STEPS_PER_EPOCH)
    predicted_labels = np.argmax(predictions, axis=1)
    # readBatch("testData")
    #截取训练的字串
    print("测试集数据读取完毕")
    global true_labels
    true_labels = true_labels[:VALIDATION_STEPS_PER_EPOCH*BATCH_SIZE]
    #转换成
    true_labels = true_labels.astype(np.int64)
    predicted_labels = predicted_labels.astype(np.int64)
    global train_time
    train_time += end - start
    target_names = ["Ewind","plankton","Nandrobox","Biige","VirusShield","FakeAV","AndroidSpy.277","WannaLocker","RansomBO","LockerPin","Charger","Youmi","Kemoge"]

    result = classification_report(true_labels, predicted_labels, target_names=target_names,digits=5)
    acc = metrics.accuracy_score(y_true=true_labels, y_pred=predicted_labels)
    print("准确率")
    print(acc)
    #绘制acc-loss曲线
    # history.loss_plot('epoch')
    print(result)
    C2 = metrics.confusion_matrix(true_labels, predicted_labels, labels=[0,1,2,4,5,6,8,9,10,11,12,14,15])
    # plot_Matrix(C2,target_names)
    return acc,result


def save_result(acc,result):
    with open('malware_11class.txt','a') as f:
        f.write("\n")
        f.write('CLASS_NUM: 11\n')
        f.write('MINI_BATCH: ' + str(BATCH_SIZE) + "\n")
        f.write('TRAIN_EPOCHS: ' + str(TRAIN_EPOCHS) + "\n")
        f.write('TRAIN_TIME: ' + str(train_time) + "\n")
        f.write('acc: ' + str(acc) + "\n")
        f.write('result: ' + str(result) + "\n")

if __name__ == "__main__":
    # useGpus()
    train_data_generator = mini_batch_generator("trainData")
    test_data_generator  = mini_batch_generator("testData")
    valid_data_generator  = mini_batch_generator("validData")
    print("数据读取完毕")
    model = makeModel()
    acc,result = train(model)
    save_result(acc,result)
