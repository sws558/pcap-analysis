import datetime   #二分类模型
from timeit import default_timer as timer

import keras.callbacks
import numpy as np
import tensorflow as tf
from keras.layers import (GRU, Bidirectional, Conv1D, Dense, Input, Lambda,
                          MaxPooling1D, TimeDistributed)
from keras.models import Model

from AttentionNetwork import HierarchicalAttentionNetwork

data_path = '/home/lenovo/opt/pcap_analysis'#？？？？
PACKET_NUM_PER_SESSION = 30
PACKET_LEN = 300
# k_valid_num = 10
BATCH_SIZE = 64
LSTM_UNITS = 92

TRAIN_STEPS_PER_EPOCH = 2122  # 74793  每个epoch分成多少个batch，注意！！！！！！！！！！！！！！！！！！！！！！
TEST_STEPS_PER_EPOCH = 265
VALIDATION_STEPS_PER_EPOCH = 265  # 28046
TRAIN_EPOCHS = 20
true_labels = []
train_time = 0

# 读取数据集,datatype是在数据集中key值


def readBatch(dataType):
    print("开始读取数据")
    print(dataType)

    path = data_path+"/test_data.npz"

    data = np.load(path, allow_pickle=True)
    pcapdata = []
    for i in range(len(data[dataType])):
        pcapdata.append(data['data'][i]['data'])
    pcapdata = np.array(pcapdata)
    print(pcapdata.shape)
    pcapdata = pcapdata[:, :PACKET_NUM_PER_SESSION, :PACKET_LEN]*255
    print(pcapdata.shape)
    # 数据输入转换成hast输入格式，(BATCH_SIZE, PACKET_NUM_PER_SESSION, PACKET_LEN)
    pcapdata_reshape = pcapdata.reshape(
        (-1, PACKET_NUM_PER_SESSION, PACKET_LEN))
    print(pcapdata.shape)
    print("训练集截取后的长度")
    # 这里需要进行一层转换，因为存储的时候没有对类别进行reshape
    return pcapdata_reshape


def mini_batch_generator(dataType):
    X = readBatch(dataType)
    total_size = X.shape[0]
    batch_idx = 0
    while True:
        for i in range(total_size//BATCH_SIZE):
            yield X[i*BATCH_SIZE:(i+1)*BATCH_SIZE]


def binarize(x, sz=256):
    return tf.cast(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1), dtype=tf.float32)


def binarize_outshape(in_shape):
    return in_shape[0], in_shape[1], 256


def byte_block(in_layer, nb_filter=(64, 100), filter_length=(3, 3), subsample=(2, 1), pool_length=(2, 2)):
    block = in_layer
    for i in range(len(nb_filter)):
        block = Conv1D(filters=nb_filter[i],
                       kernel_size=filter_length[i],
                       padding='valid',
                       activation='tanh',
                       strides=subsample[i])(block)
        if pool_length[i]:
            block = MaxPooling1D(pool_size=pool_length[i])(block)
    return block


test_data_generator = mini_batch_generator("data")
print("数据读取完毕")


def makeModel():
    session = Input(shape=(PACKET_NUM_PER_SESSION, PACKET_LEN), dtype='int64')
    input_packet = Input(shape=(PACKET_LEN,), dtype='int64')
    embedded = Lambda(binarize, output_shape=binarize_outshape)(input_packet)
    block = byte_block(embedded, (256, 512), filter_length=(
        6, 5), subsample=(1, 1), pool_length=(2, 2))
    attn_package = HierarchicalAttentionNetwork(192)(block)
    encoder = Model(inputs=input_packet, outputs=attn_package)
    encoder.summary()#模型1

    review_encoder = TimeDistributed(encoder)(session)
    gru_flow = Bidirectional(GRU(512, return_sequences=True))(review_encoder)
    attn_sentence = HierarchicalAttentionNetwork(512)(gru_flow)
    fullconnect = Dense(512, activation='relu')(attn_sentence)
    fullconnect = Dense(256, activation='relu')(fullconnect)
    preds = Dense(2, activation='softmax')(fullconnect)
    model = Model(outputs=preds, inputs=session)
    model.summary()#模型2
    return model


def train(model):
    now_time = datetime.datetime.now()
    model.load_weights("./save_model/_300_30_normal_malware.hdf5")
    earlystop_cb = keras.callbacks.EarlyStopping(
        monitor='accuracy', patience=100, verbose=0, mode='max')
    model.compile(loss='binary_crossentropy',
                  optimizer='rmsprop', metrics=['accuracy'])
    start = timer()
    end = timer()
    predictions = model.predict_generator(
        test_data_generator, verbose=1, steps=VALIDATION_STEPS_PER_EPOCH)
    predicted_labels = np.argmax(predictions, axis=1)
    # 截取训练的字串
    print("测试集数据读取完毕")
    predicted_labels = predicted_labels.astype(np.int64)
    global train_time
    train_time += end - start
    target_names = ["normal", "abnormal"]
    #result = classification_report(true_labels, predicted_labels, target_names=target_names)
    print(predicted_labels)
    return predicted_labels


if __name__ == "__main__":
    test_data_generator = mini_batch_generator("data")
    print("数据读取完毕")
    model = makeModel()
    predicted_labels = train(model)
